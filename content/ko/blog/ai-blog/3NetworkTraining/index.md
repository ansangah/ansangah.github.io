---
title: "📁 3장 : 신경망 학습과 최적화"
summary: 신경망이 데이터를 통해 규칙을 익히는 과정과 이를 가능하게 만드는 최적화, 경사 하강법의 핵심 아이디어를 정리합니다.
date: 2025-10-17

authors:
  - admin

tags:
  - Deep Learning
  - Optimization
  - Gradient Descent

course_topics:
  - 신경망 학습
  - 최적화 문제 정의
  - 경사 하강법

content_meta:
  trending: true

exclude_search: false
math: true
dl_kind: "textbook"
---

신경망 학습은 **경험 데이터로부터 규칙을 찾아 추론 능력을 갖추는 과정**이다. 이 장에서는 신경망이 학습을 통해 무엇을 얻는지, 이를 위해 어떻게 최적화 문제를 정의하고 해결하는지, 그리고 핵심 알고리즘인 경사 하강법이 어떻게 작동하는지를 정리한다.

{{< toc mobile_only=true is_open=true >}}

## 3.1 신경망 학습의 의미

- 신경망은 초기에는 입력이 들어와도 올바른 출력을 만들 수 없다.  
  학습 데이터를 반복적으로 주입하면서 **입·출력 매핑 규칙**을 스스로 찾아야 한다.
- 신경망 구조(은닉층 수, 뉴런 수, 활성 함수 등)는 학습 전에 사람이 정하는 **하이퍼파라미터**다.  
  반면 학습이 진행되면서 자동으로 조정되는 값은 **모델 파라미터**(가중치, 편향)다.
- 모델 파라미터는 데이터에 의존해 계속 변경되며, 학습이 끝나면 모델 상태로 저장되어 추론 시 사용된다.

| 구분 | 예시 | 특징 |
| --- | --- | --- |
| 모델 파라미터 | 가중치, 편향 | 학습 과정에서 자동으로 업데이트되어 모델에 저장됨 |
| 하이퍼파라미터 | 학습률, 옵티마이저, 배치 크기, Dropout 확률, 네트워크 깊이 등 | 사람이 사전에 설정, 모델이 직접 학습하지 않음 |

가령 집값 예측 문제에서 `방 개수·면적·집 종류·역과의 거리 → 집값` 관계를 학습하면, 신경망은 모든 뉴런의 가중치와 편향을 통해 이 규칙을 함수 형태로 표현한다. 학습이 종료되면 새로운 입력에 대해 추론(예측)이 가능해진다.  
핵심 질문은 **“어떤 방법으로 최적의 파라미터 값을 찾을 것인가?”**이며, 해답은 최적화 알고리즘이다.

## 3.2 신경망 학습과 최적화

### 최적화란?

- 유한한 방정식만으로 정확한 해를 구하기 어려운 문제에서 근사 해를 반복적으로 찾는 방법.
- 목적 함수를 최대화·최소화하면서 제약 조건을 만족하는 해를 탐색한다.

표준 최적화 문제는 다음과 같이 표현한다.

$$
\begin{aligned}
&\text{minimize} && f(\mathbf{x}) \\
&\text{subject to} && g_i(\mathbf{x}) \le 0,\quad i=1,\dots,m \\
&&& h_j(\mathbf{x}) = 0,\quad j=1,\dots,p
\end{aligned}
$$

- $f(\mathbf{x})$: 목적(손실) 함수  
- $g_i, h_j$: 부등식/등식 제약  
- $\mathbf{x}$: 최적해를 찾아야 하는 변수

최적해에 가까워지는 과정을 **수렴**, 최종적으로 도달하면 **수렴했다**고 말한다.  
최소화 문제의 목적 함수는 비용(cost) 또는 손실(loss) 함수, 최대화 문제는 유틸리티 함수로 부르기도 한다.

### 신경망에서의 최적화 문제

- **회귀 문제**: 타깃 $t$와 예측 $y = f_\theta(x)$의 차이를 최소화하는 파라미터 $\theta$를 찾는다.  
  대표 손실 함수는 평균제곱오차(MSE).

  $$
  \mathcal{L}_{\text{MSE}}(\theta) = \frac{1}{N}\sum_{n=1}^{N} \|t_n - f_\theta(x_n)\|^2
  $$

- **분류 문제**: 관측 확률분포 $t$와 예측 확률분포 $y$의 차이를 최소화하는 파라미터를 찾는다.  
  대표 손실 함수는 크로스 엔트로피.

  $$
  \mathcal{L}_{\text{CE}}(\theta) = -\frac{1}{N}\sum_{n=1}^{N} \sum_{k=1}^{K} t_{n,k} \log y_{n,k}
  $$

최적화는 손실 함수의 형태(회귀인지 분류인지)에 따라 정의한 뒤, 그 손실을 최소화하는 방향으로 파라미터를 갱신하도록 설계된다.

## 3.3 경사 하강법

### 목표: 좋은 지역 최소 찾기

- 전역 최소(global minimum)를 찾으면 좋지만, 실제 신경망 손실 곡면은 매우 복잡하고 고차원이라 계산 비용이 매우 크다.
- 현실적으로는 **여러 지역 최소 중 충분히 좋은 지점**을 찾는 전략을 취한다. 필요하다면 다른 초기값으로 학습을 반복해 가장 낮은 손실을 선택한다.

### 최적화 알고리즘 분류

손실 함수의 곡면을 얼마나 정교하게 근사하느냐에 따라 1차, 1.5차, 2차 미분 기반 알고리즘으로 나눌 수 있다.

- **1차 미분 기반**: 경사 하강법(Gradient Descent), SGD, Momentum, AdaGrad, RMSProp, Adam 등  
  - 곡면이 볼록하지 않아도 안정적으로 동작하며, 신경망 최적화의 기본 도구.
- **1.5차 미분 기반**: 준-뉴턴(BFGS), 켤레 경사, 레벤버그-마쿼트 등  
  - 1차 정보로 2차 곡률을 근사해 빠르게 수렴하지만 메모리 사용량이 높다.
- **2차 미분 기반**: 뉴턴 방법, 내부점법 등  
  - 곡률 정보를 직접 사용해 빠르게 수렴하지만, 손실이 볼록해야 하고 계산 비용이 커서 대규모 신경망에 적용하기 어렵다.

### 경사 하강법의 아이디어

손실 함수 $L(\theta)$의 기울기를 따라 파라미터를 반복 갱신한다.

$$
\theta^{(t+1)} = \theta^{(t)} - \alpha \nabla_\theta L(\theta^{(t)})
$$

- $\alpha$: 학습률(learning rate). 한 번에 이동하는 step 크기를 조절한다.
- $-\nabla_\theta L$: 현재 지점에서 가장 가파르게 내려가는 방향.

기울기 $\nabla_\theta L$은 손실을 파라미터에 대해 편미분한 값이며, 실수 함수가 가장 빠르게 증가하는 방향과 기울기를 알려준다. 음수 방향으로 이동하면 손실이 감소하는 방향으로 내려간다는 뜻이다.

신경망은 여러 층을 합성한 함수 구조이므로, 경사 하강법을 적용할 때 **연쇄 법칙(chain rule)**을 이용해 각 층의 그레이디언트를 계산한다. 이 과정이 바로 역전파(backpropagation)다.

### 적용 예시

- 2계층 회귀 신경망 (은닉층 ReLU, 출력층 항등 함수, 손실 MSE)을 생각해 보자.
- 학습 과정은 다음을 반복한다.
  1. 순전파로 예측값 계산.
  2. 손실 함수(MSE) 평가.
  3. 손실을 각 파라미터에 대해 편미분(역전파).
  4. 경사 하강법 업데이트 식으로 가중치·편향 갱신.

손실이 일정 임계치 이하로 더 이상 줄어들지 않으면 수렴했다고 판단하고 학습을 종료한다. 이렇게 학습된 파라미터는 입력이 될 때마다 적절한 출력을 생성할 수 있는 **함수적 규칙**을 제공한다.

---

요약하면, 신경망 학습은 데이터를 통해 규칙을 학습하는 과정이며, 이 과정을 수학적으로는 손실 함수 최소화 문제로 바라본다. 경사 하강법과 같은 최적화 기법이 파라미터를 반복적으로 조정해 모델이 점점 더 정확한 추론을 수행하도록 만든다.

---
{{< button url="/uploads/deeplearning/3-신경망학습.pdf" style="primary" size="lg" icon="document-arrow-down" align="center" >}}
PDF 다운로드
{{< /button >}}
